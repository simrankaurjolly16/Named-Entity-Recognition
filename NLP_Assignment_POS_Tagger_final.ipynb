{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_POS_Tagger",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwUwhn5KidE",
        "colab_type": "code",
        "outputId": "ae59fd4c-e961-428e-d156-c4935e2855c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md36KiBfKpz5",
        "colab_type": "code",
        "outputId": "5faee643-8981-438c-f301-f6a8415ddcb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-05 04:54:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-05 04:54:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-05 04:54:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.10MB/s    in 6m 27s  \n",
            "\n",
            "2020-05-05 05:01:07 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4wFBm0hKxo_",
        "colab_type": "code",
        "outputId": "187b3b00-296b-45c9-ee8f-fbf12a2b0e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir data/\n",
        "!unzip glove.6B.zip\n",
        "!mv glove.6B.50d.txt data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd9dhHkNK30A",
        "colab_type": "code",
        "outputId": "0178cf66-d753-494c-a58d-eac9e4e5c35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install conllu\n",
        "!git clone https://github.com/pasinit/nlp2020_POStagging_data.git\n",
        "!unzip nlp2020_POStagging_data/r2.2.zip  > /dev/null\n",
        "!rm -rf nlp2020_POStagging_data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/03/4a952eb39cdc8da80a6a2416252e71784dda6bf9d726ab98065fff2aeb73/conllu-2.3.2-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-2.3.2\n",
            "Cloning into 'nlp2020_POStagging_data'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqKinmmyLHJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "# from torchtext import data\n",
        "# from torchtext.vocab import Vectors\n",
        "from collections import defaultdict\n",
        "from conllu import parse as conllu_parse\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "# from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 123456\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# from torchtext.vocab import Vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzjvdE7zLPk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_file = \"drive/My Drive/NLP_Assignment/train.tsv\"\n",
        "dev_file = \"drive/My Drive/NLP_Assignment/dev.tsv\"\n",
        "test_file = \"drive/My Drive/NLP_Assignment/test.tsv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4RamS6NLSka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class man_made_Vocab(object):\n",
        "    \n",
        "    UNK = \"<unk>\"\n",
        "    \n",
        "    def __init__(self, counter, min_freq=1, specials=['<unk>', '<pad>']):\n",
        "        \n",
        "        counter = counter.copy()\n",
        "        self.itos = list()\n",
        "        self.unk_index = None\n",
        "        self.itos = list(specials)\n",
        "        \n",
        "        for tok in specials:\n",
        "            del counter[tok]\n",
        "            \n",
        "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        \n",
        "        for word, freq in words_and_frequencies:\n",
        "            self.itos.append(word)\n",
        "        \n",
        "        if man_made_Vocab.UNK in specials:\n",
        "            unk_index = specials.index(man_made_Vocab.UNK)\n",
        "            self.unk_index = unk_index\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "        else:\n",
        "            self.stoi = defaultdict()\n",
        "        \n",
        "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
        "        \n",
        "    def _default_unk_index(self):\n",
        "        return self.unk_index\n",
        "\n",
        "    def __getstate__(self):\n",
        "        # avoid picking defaultdict\n",
        "        attrs = dict(self.__dict__)\n",
        "        # cast to regular dict\n",
        "        attrs['stoi'] = dict(self.stoi)\n",
        "        return attrs\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if state.get(\"unk_index\", None) is None:\n",
        "            stoi = defaultdict()\n",
        "        else:\n",
        "            stoi = defaultdict(self._default_unk_index)\n",
        "        stoi.update(state['stoi'])\n",
        "        state['stoi'] = stoi\n",
        "        self.__dict__.update(state)\n",
        "    \n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi.get(man_made_Vocab.UNK))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1_L_1AvLZAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTaggingDataset(Dataset):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_file:str, \n",
        "                 window_size:int, \n",
        "                 window_shift:int=-1,\n",
        "                 lowercase=True, \n",
        "                 device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        We assume that the dataset pointed by input_file is already tokenized \n",
        "        and can fit in memory.\n",
        "        Args:\n",
        "            input_file (string): The path to the dataset to be loaded.\n",
        "            window_size (integer): The maximum length of a sentence in terms of \n",
        "            number of tokens.\n",
        "            window_shift (integer): The number of tokens we shift the window \n",
        "            over the sentence. Default value is -1 meaning that the window will\n",
        "            be shifted by window_size.\n",
        "            lowercase (boolean): whether the text has to be lowercased or not.\n",
        "            device (string): device where to put tensors (cpu or cuda).\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_file = input_file\n",
        "        self.window_size = window_size\n",
        "        self.window_shift = window_shift if window_shift > 0 else window_size\n",
        "        self.lowercase = lowercase\n",
        "        with open(input_file) as reader:\n",
        "            # read the entire file with reader.read() e parse it\n",
        "            sentences = conllu_parse(reader.read())\n",
        "        self.device = device\n",
        "        self.data = self.create_windows(sentences)\n",
        "        self.encoded_data = None\n",
        "    \n",
        "    def index_dataset(self, l_vocabulary, l_label_vocabulary):\n",
        "        self.encoded_data = list()\n",
        "        for i in range(len(self.data)):\n",
        "            # for each window\n",
        "            elem = self.data[i]\n",
        "            encoded_elem = torch.LongTensor(self.encode_text(elem, l_vocabulary)).to(self.device)\n",
        "            # for each element d in the elem window (d is a dictionary with the various fields from the CoNLL line) \n",
        "            encoded_labels = torch.LongTensor([l_label_vocabulary.stoi[d[\"lemma\"]] if d is not None \n",
        "                              else l_label_vocabulary.stoi[\"<pad>\"] for d in elem]).to(self.device)\n",
        "            self.encoded_data.append({\"inputs\":encoded_elem, \"outputs\":encoded_labels})\n",
        "\n",
        "    def create_windows(self, sentences):\n",
        "        \"\"\" \n",
        "        Args:\n",
        "            sentences (list of lists of dictionaries, \n",
        "                          where each dictionary represents a word occurrence parsed from a CoNLL line)\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        for sentence in sentences:\n",
        "            if self.lowercase:\n",
        "                for d in sentence:\n",
        "                    # lowers the inflected form\n",
        "                    d[\"form\"] = d[\"form\"].lower()\n",
        "            for i in range(0, len(sentence), self.window_shift):\n",
        "                window = sentence[i:i+self.window_size]\n",
        "                if len(window) < self.window_size:\n",
        "                    window = window + [None]*(self.window_size - len(window))\n",
        "                assert len(window) == self.window_size\n",
        "                data.append(window)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.encoded_data is None:\n",
        "            raise RuntimeError(\"\"\"Trying to retrieve elements but index_dataset\n",
        "            has not been invoked yet! Be sure to invoce index_dataset on this object\n",
        "            before trying to retrieve elements. In case you want to retrieve raw\n",
        "            elements, use the method get_raw_element(idx)\"\"\")\n",
        "        return self.encoded_data[idx]\n",
        "    \n",
        "    def get_raw_element(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_text(sentence:list, l_vocabulary):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentences (list): list of OrderedDict, each carrying the information about\n",
        "            one token.\n",
        "            l_vocabulary (Vocab): vocabulary with mappings from words to indices and viceversa.\n",
        "        Return:\n",
        "            The method returns a list of indices corresponding to the input tokens.\n",
        "        \"\"\"\n",
        "        indices = list()\n",
        "        for w in sentence:\n",
        "            if w is None:\n",
        "                indices.append(l_vocabulary.stoi[\"<pad>\"])\n",
        "            elif w[\"form\"] in l_vocabulary.stoi: # vocabulary string to integer\n",
        "                indices.append(l_vocabulary.stoi[w[\"form\"]])\n",
        "            else:\n",
        "                indices.append(l_vocabulary.stoi[\"<unk>\"])\n",
        "        return indices\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode_output(outputs:torch.Tensor,\n",
        "                    l_label_vocabulary):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            outputs (Tensor): a Tensor with shape (batch_size, max_len, label_vocab_size)\n",
        "                containing the logits outputed by the neural network.\n",
        "            l_label_vocabulary (Vocab): is the vocabulary containing the mapping from\n",
        "            a string label to its corresponding index and vice versa\n",
        "        Output:\n",
        "            The method returns a list of batch_size length where each element is a list\n",
        "            of labels, one for each input token.\n",
        "        \"\"\"\n",
        "        max_indices = torch.argmax(outputs, -1).tolist() # shape = (batch_size, max_len)\n",
        "        predictions = list()\n",
        "        for indices in max_indices:\n",
        "            # vocabulary integer to string is used to obtain the corresponding word from the max index\n",
        "            predictions.append([l_label_vocabulary.itos[i] for i in indices])\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJXup0GGLcAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(dataset, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        # for each token in the sentence viewed as a dictionary of items from the CoNLL line\n",
        "        for token in dataset.get_raw_element(i):\n",
        "            if token is not None:\n",
        "                counter[token[\"form\"]]+=1\n",
        "    # we add special tokens for handling padding and unknown words at testing time.\n",
        "    return man_made_Vocab(counter, min_freq=min_freq, specials=['<pad>', '<unk>'])\n",
        "\n",
        "def build_label_vocab(dataset):\n",
        "    counter = Counter()\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        for token in dataset.get_raw_element(i):\n",
        "            if token is not None:\n",
        "                counter[token[\"lemma\"]]+=1\n",
        "    # No <unk> token for labels.\n",
        "    return man_made_Vocab(counter, specials=['<pad>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpfhwrCU6ybB",
        "colab_type": "code",
        "outputId": "cb2942d4-5541-4a07-995e-f6adce815812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "window_size, window_shift = 100, 100\n",
        "dataset = POSTaggingDataset(training_file, window_size, window_shift)\n",
        "vocabulary = build_vocab(dataset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(dataset)\n",
        "dataset.index_dataset(vocabulary, label_vocabulary)\n",
        "print(len(vocabulary))\n",
        "print(len(label_vocabulary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100042/100042 [00:02<00:00, 49628.17it/s]\n",
            "100%|██████████| 100042/100042 [00:01<00:00, 85494.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "91582\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJsU6aRBLeD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import six\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "def reporthook(t):\n",
        "    \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
        "    last_b = [0]\n",
        "\n",
        "    def inner(b=1, bsize=1, tsize=None):\n",
        "        \"\"\"\n",
        "        b: int, optional\n",
        "        Number of blocks just transferred [default: 1].\n",
        "        bsize: int, optional\n",
        "        Size of each block (in tqdm units) [default: 1].\n",
        "        tsize: int, optional\n",
        "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
        "        \"\"\"\n",
        "        if tsize is not None:\n",
        "            t.total = tsize\n",
        "        t.update((b - last_b[0]) * bsize)\n",
        "        last_b[0] = b\n",
        "    return inner\n",
        "  \n",
        "def _infer_shape(f):\n",
        "    num_lines, vector_dim = 0, None\n",
        "    for line in f:\n",
        "        if vector_dim is None:\n",
        "            row = line.rstrip().split(b\" \")\n",
        "            vector = row[1:]\n",
        "            # Assuming word, [vector] format\n",
        "            if len(vector) > 2:\n",
        "                # The header present in some (w2v) formats contains two elements.\n",
        "                vector_dim = len(vector)\n",
        "                num_lines += 1  # First element read\n",
        "        else:\n",
        "            num_lines += 1\n",
        "    f.seek(0)\n",
        "    return num_lines, vector_dim\n",
        "\n",
        "class Vectorization(object):\n",
        "\n",
        "    def __init__(self, name, cache=None,\n",
        "                 url=None, unk_init=None, max_vectors=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "           name: name of the file that contains the vectors\n",
        "           cache: directory for cached vectors\n",
        "           url: url for download if vectors not found in cache\n",
        "           unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "               to zero vectors; can be any function that takes in a Tensor and\n",
        "               returns a Tensor of the same size\n",
        "           max_vectors (int): this can be used to limit the number of\n",
        "               pre-trained vectors loaded.\n",
        "               Most pre-trained vector sets are sorted\n",
        "               in the descending order of word frequency.\n",
        "               Thus, in situations where the entire set doesn't fit in memory,\n",
        "               or is not needed for another reason, passing `max_vectors`\n",
        "               can limit the size of the loaded set.\n",
        "        \"\"\"\n",
        "        cache = '.vector_cache' if cache is None else cache\n",
        "        self.itos = None\n",
        "        self.stoi = None\n",
        "        self.vectors = None\n",
        "        self.dim = None\n",
        "        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n",
        "        self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        if token in self.stoi:\n",
        "            return self.vectors[self.stoi[token]]\n",
        "        else:\n",
        "            return self.unk_init(torch.Tensor(self.dim))\n",
        "\n",
        "    def cache(self, name, cache, url=None, max_vectors=None):\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        if os.path.isfile(name):\n",
        "            path = name\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n",
        "        else:\n",
        "            path = os.path.join(cache, name)\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = path + file_suffix\n",
        "\n",
        "        if not os.path.isfile(path_pt):\n",
        "            if not os.path.isfile(path) and url:\n",
        "                # logger.info('Downloading vectors from {}'.format(url))\n",
        "                if not os.path.exists(cache):\n",
        "                    os.makedirs(cache)\n",
        "                dest = os.path.join(cache, os.path.basename(url))\n",
        "                if not os.path.isfile(dest):\n",
        "                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n",
        "                        try:\n",
        "                            urlretrieve(url, dest, reporthook=reporthook(t))\n",
        "                        except KeyboardInterrupt as e:  # remove the partial zip file\n",
        "                            os.remove(dest)\n",
        "                            raise e\n",
        "                # logger.info('Extracting vectors into {}'.format(cache))\n",
        "                ext = os.path.splitext(dest)[1][1:]\n",
        "                if ext == 'zip':\n",
        "                    with zipfile.ZipFile(dest, \"r\") as zf:\n",
        "                        zf.extractall(cache)\n",
        "                elif ext == 'gz':\n",
        "                    if dest.endswith('.tar.gz'):\n",
        "                        with tarfile.open(dest, 'r:gz') as tar:\n",
        "                            tar.extractall(path=cache)\n",
        "            if not os.path.isfile(path):\n",
        "                raise RuntimeError('no vectors found at {}'.format(path))\n",
        "\n",
        "            # logger.info(\"Loading vectors from {}\".format(path))\n",
        "            ext = os.path.splitext(path)[1][1:]\n",
        "            if ext == 'gz':\n",
        "                open_file = gzip.open\n",
        "            else:\n",
        "                open_file = open\n",
        "\n",
        "            vectors_loaded = 0\n",
        "            with open_file(path, 'rb') as f:\n",
        "                num_lines, dim = _infer_shape(f)\n",
        "                if not max_vectors or max_vectors > num_lines:\n",
        "                    max_vectors = num_lines\n",
        "\n",
        "                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n",
        "\n",
        "                for line in tqdm(f, total=max_vectors):\n",
        "                    # Explicitly splitting on \" \" is important, so we don't\n",
        "                    # get rid of Unicode non-breaking spaces in the vectors.\n",
        "                    entries = line.rstrip().split(b\" \")\n",
        "\n",
        "                    word, entries = entries[0], entries[1:]\n",
        "                    if dim is None and len(entries) > 1:\n",
        "                        dim = len(entries)\n",
        "                    elif len(entries) == 1:\n",
        "                        # logger.warning(\"Skipping token {} with 1-dimensional \"\n",
        "                                      #  \"vector {}; likely a header\".format(word, entries))\n",
        "                        continue\n",
        "                    elif dim != len(entries):\n",
        "                        raise RuntimeError(\n",
        "                            \"Vector for token {} has {} dimensions, but previously \"\n",
        "                            \"read vectors have {} dimensions. All vectors must have \"\n",
        "                            \"the same number of dimensions.\".format(word, len(entries),\n",
        "                                                                    dim))\n",
        "\n",
        "                    try:\n",
        "                        if isinstance(word, six.binary_type):\n",
        "                            word = word.decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        # logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
        "                        continue\n",
        "\n",
        "                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n",
        "                    vectors_loaded += 1\n",
        "                    itos.append(word)\n",
        "\n",
        "                    if vectors_loaded == max_vectors:\n",
        "                        break\n",
        "\n",
        "            self.itos = itos\n",
        "            self.stoi = {word: i for i, word in enumerate(itos)}\n",
        "            self.vectors = torch.Tensor(vectors).view(-1, dim)\n",
        "            self.dim = dim\n",
        "            # logger.info('Saving vectors to {}'.format(path_pt))\n",
        "            if not os.path.exists(cache):\n",
        "                os.makedirs(cache)\n",
        "            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n",
        "        else:\n",
        "            # logger.info('Loading vectors from {}'.format(path_pt))\n",
        "            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vectors)\n",
        "\n",
        "    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n",
        "        \"\"\"Look up embedding vectors of tokens.\n",
        "\n",
        "        Arguments:\n",
        "            tokens: a token or a list of tokens. if `tokens` is a string,\n",
        "                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n",
        "                list of strings, returns a 2-D tensor of shape=(len(tokens),\n",
        "                self.dim).\n",
        "            lower_case_backup : Whether to look up the token in the lower case.\n",
        "                If False, each token in the original case will be looked up;\n",
        "                if True, each token in the original case will be looked up first,\n",
        "                if not found in the keys of the property `stoi`, the token in the\n",
        "                lower case will be looked up. Default: False.\n",
        "\n",
        "        Examples:\n",
        "            >>> examples = ['chip', 'baby', 'Beautiful']\n",
        "            >>> vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "        \"\"\"\n",
        "        to_reduce = False\n",
        "\n",
        "        if not isinstance(tokens, list):\n",
        "            tokens = [tokens]\n",
        "            to_reduce = True\n",
        "\n",
        "        if not lower_case_backup:\n",
        "            indices = [self[token] for token in tokens]\n",
        "        else:\n",
        "            indices = [self[token] if token in self.stoi\n",
        "                       else self[token.lower()]\n",
        "                       for token in tokens]\n",
        "\n",
        "        vecs = torch.stack(indices)\n",
        "        return vecs[0] if to_reduce else vecs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jPhVHCMbUMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTaggerModel(nn.Module):\n",
        "    # we provide the hyperparameters as input\n",
        "    def __init__(self, hparams):\n",
        "        super(POSTaggerModel, self).__init__()\n",
        "        # Embedding layer: a mat∂rix vocab_size x embedding_dim where each index \n",
        "        # correspond to a word in the vocabulary and the i-th row corresponds to \n",
        "        # a latent representation of the i-th word in the vocabulary.\n",
        "        pprint(params)\n",
        "        self.word_embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n",
        "        if hparams.embeddings is not None:\n",
        "            print(\"initializing embeddings from pretrained\")\n",
        "            self.word_embedding.weight.data.copy_(hparams.embeddings)\n",
        "\n",
        "        # LSTM layer: an LSTM neural network that process the input text\n",
        "        # (encoded with word embeddings) from left to right and outputs \n",
        "        # a new **contextual** representation of each word that depend\n",
        "        # on the preciding words.\n",
        "        self.lstm = nn.LSTM(hparams.embedding_dim, hparams.hidden_dim, \n",
        "                            bidirectional=hparams.bidirectional,\n",
        "                            num_layers=hparams.num_layers, \n",
        "                            dropout = hparams.dropout if hparams.num_layers > 1 else 0)\n",
        "        # Hidden layer: transforms the input value/scalar into\n",
        "        # a hidden vector representation.\n",
        "        lstm_output_dim = hparams.hidden_dim if hparams.bidirectional is False else hparams.hidden_dim * 2\n",
        "\n",
        "        # During training, randomly zeroes some of the elements of the \n",
        "        # input tensor with probability hparams.dropout using samples \n",
        "        # from a Bernoulli distribution. Each channel will be zeroed out \n",
        "        # independently on every forward call.\n",
        "        # This has proven to be an effective technique for regularization and \n",
        "        # preventing the co-adaptation of neurons\n",
        "        self.dropout = nn.Dropout(hparams.dropout)\n",
        "        self.classifier = nn.Linear(lstm_output_dim, hparams.num_classes)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embeddings = self.word_embedding(x)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        o, (h, c) = self.lstm(embeddings)\n",
        "        o = self.dropout(o)\n",
        "        output = self.classifier(o)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNBr9gVScZlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "    \"\"\"Utility class to train and evaluate a model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        label_vocab,\n",
        "        log_steps:int=10_000,\n",
        "        log_level:int=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: the model we want to train.\n",
        "            loss_function: the loss_function to minimize.\n",
        "            optimizer: the optimizer used to minimize the loss_function.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.label_vocab = label_vocab\n",
        "        self.log_steps = log_steps\n",
        "        self.log_level = log_level\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def train(self, train_dataset:Dataset, \n",
        "              valid_dataset:Dataset, \n",
        "              epochs:int=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            train_dataset: a Dataset or DatasetLoader instance containing\n",
        "                the training instances.\n",
        "            valid_dataset: a Dataset or DatasetLoader instance used to evaluate\n",
        "                learning progress.\n",
        "            epochs: the number of times to iterate over train_dataset.\n",
        "\n",
        "        Returns:\n",
        "            avg_train_loss: the average training loss on train_dataset over\n",
        "                epochs.\n",
        "        \"\"\"\n",
        "        assert epochs > 1 and isinstance(epochs, int)\n",
        "        if self.log_level > 0:\n",
        "            print('Training ...')\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            if self.log_level > 0:\n",
        "                print(' Epoch {:03d}'.format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                sample_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += sample_loss.tolist()\n",
        "\n",
        "                if self.log_level > 1 and step % self.log_steps == self.log_steps - 1:\n",
        "                    print('\\t[E: {:2d} @ step {}] current avg loss = {:0.4f}'.format(epoch, step, epoch_loss / (step + 1)))\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "            train_loss += avg_epoch_loss\n",
        "            if self.log_level > 0:\n",
        "                print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            valid_loss = self.evaluate(valid_dataset)\n",
        "            \n",
        "            if self.log_level > 0:\n",
        "                print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
        "\n",
        "        if self.log_level > 0:\n",
        "            print('... Done!')\n",
        "        \n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss\n",
        "    \n",
        "\n",
        "    def evaluate(self, valid_dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            valid_dataset: the dataset to use to evaluate the model.\n",
        "\n",
        "        Returns:\n",
        "            avg_valid_loss: the average validation loss over valid_dataset.\n",
        "        \"\"\"\n",
        "        valid_loss = 0.0\n",
        "        # set dropout to 0!! Needed when we are in inference mode.\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sample in valid_dataset:\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                valid_loss += sample_loss.tolist()\n",
        "        \n",
        "        return valid_loss / len(valid_dataset)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: a tensor of indices.\n",
        "        Returns: \n",
        "            A list containing the predicted POS tag for each token in the\n",
        "            input sentences.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(x)\n",
        "            predictions = torch.argmax(logits, -1)\n",
        "            return logits, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13N_rpKSbYfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HParams():\n",
        "    vocab_size = len(vocabulary)\n",
        "    hidden_dim = 128\n",
        "    embedding_dim = 100\n",
        "    num_classes = len(label_vocabulary) # number of different universal POS tags\n",
        "    bidirectional = True\n",
        "    num_layers = 2\n",
        "    dropout = 0.0\n",
        "    embeddings = None\n",
        "params = HParams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFMnOV9dba_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "vectors = Vectorization(\"glove.6B.100d.txt\")\n",
        "pretrained_embeddings = torch.randn(len(vocabulary), vectors.dim)\n",
        "initialised = 0\n",
        "for i, w in enumerate(vocabulary.itos):\n",
        "    if w in vectors.stoi:\n",
        "        initialised += 1\n",
        "        vec = vectors.get_vecs_by_tokens(w)\n",
        "        pretrained_embeddings[i] = vec\n",
        "    \n",
        "pretrained_embeddings[vocabulary[\"<pad>\"]] = torch.zeros(vectors.dim)\n",
        "params.embedding_dim=vectors.dim\n",
        "params.embeddings = pretrained_embeddings\n",
        "params.vocab_size = len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkJV3skwbeyT",
        "colab_type": "code",
        "outputId": "2778336f-6553-4955-e703-5b18ce706fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "window_size, window_shift = 100, 100\n",
        "device = \"cuda\"\n",
        "trainingset = POSTaggingDataset(training_file, window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(trainingset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(trainingset)\n",
        "trainingset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "devset = POSTaggingDataset(dev_file, window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(devset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(devset)\n",
        "devset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "testset = POSTaggingDataset(test_file, window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(testset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(testset)\n",
        "testset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "train_dataset = DataLoader(trainingset, batch_size=128)\n",
        "valid_dataset = DataLoader(devset, batch_size=128)\n",
        "test_dataset = DataLoader(testset, batch_size=128)\n",
        "\n",
        "postagger = POSTaggerModel(params).cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100042/100042 [00:02<00:00, 49371.11it/s]\n",
            "100%|██████████| 100042/100042 [00:01<00:00, 85523.49it/s]\n",
            "100%|██████████| 14439/14439 [00:00<00:00, 49562.05it/s]\n",
            "100%|██████████| 14439/14439 [00:00<00:00, 76007.11it/s]\n",
            "100%|██████████| 15480/15480 [00:00<00:00, 53056.97it/s]\n",
            "100%|██████████| 15480/15480 [00:00<00:00, 81367.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<__main__.HParams object at 0x7f5dc0179518>\n",
            "initializing embeddings from pretrained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSaSjrK8cJhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm_trainer = Trainer(\n",
        "    model = postagger,\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=label_vocabulary[\"<pad>\"]),\n",
        "    optimizer = optim.Adam(postagger.parameters()),\n",
        "    label_vocab=label_vocabulary\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuP9rYKcMS8",
        "colab_type": "code",
        "outputId": "adbb9e29-6ace-4d9d-8659-707f2dee90ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bilstm_trainer.train(train_dataset, valid_dataset, 20) #EPOCH1:13:44"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            " Epoch 001\n",
            "\t[E:  0] train loss = 0.1428\n",
            "  [E:  0] valid loss = 0.7351\n",
            " Epoch 002\n",
            "\t[E:  1] train loss = 0.1291\n",
            "  [E:  1] valid loss = 0.7931\n",
            " Epoch 003\n",
            "\t[E:  2] train loss = 0.1215\n",
            "  [E:  2] valid loss = 0.8496\n",
            " Epoch 004\n",
            "\t[E:  3] train loss = 0.1158\n",
            "  [E:  3] valid loss = 0.9181\n",
            " Epoch 005\n",
            "\t[E:  4] train loss = 0.1097\n",
            "  [E:  4] valid loss = 1.0028\n",
            " Epoch 006\n",
            "\t[E:  5] train loss = 0.1016\n",
            "  [E:  5] valid loss = 1.1282\n",
            " Epoch 007\n",
            "\t[E:  6] train loss = 0.0913\n",
            "  [E:  6] valid loss = 1.2931\n",
            " Epoch 008\n",
            "\t[E:  7] train loss = 0.0797\n",
            "  [E:  7] valid loss = 1.5219\n",
            " Epoch 009\n",
            "\t[E:  8] train loss = 0.0685\n",
            "  [E:  8] valid loss = 1.7686\n",
            " Epoch 010\n",
            "\t[E:  9] train loss = 0.0602\n",
            "  [E:  9] valid loss = 1.9682\n",
            " Epoch 011\n",
            "\t[E: 10] train loss = 0.0538\n",
            "  [E: 10] valid loss = 2.2911\n",
            " Epoch 012\n",
            "\t[E: 11] train loss = 0.0462\n",
            "  [E: 11] valid loss = 2.5962\n",
            " Epoch 013\n",
            "\t[E: 12] train loss = 0.0392\n",
            "  [E: 12] valid loss = 2.8177\n",
            " Epoch 014\n",
            "\t[E: 13] train loss = 0.0340\n",
            "  [E: 13] valid loss = 2.9848\n",
            " Epoch 015\n",
            "\t[E: 14] train loss = 0.0296\n",
            "  [E: 14] valid loss = 3.1320\n",
            " Epoch 016\n",
            "\t[E: 15] train loss = 0.0260\n",
            "  [E: 15] valid loss = 3.2806\n",
            " Epoch 017\n",
            "\t[E: 16] train loss = 0.0230\n",
            "  [E: 16] valid loss = 3.3571\n",
            " Epoch 018\n",
            "\t[E: 17] train loss = 0.0200\n",
            "  [E: 17] valid loss = 3.5742\n",
            " Epoch 019\n",
            "\t[E: 18] train loss = 0.0172\n",
            "  [E: 18] valid loss = 3.7393\n",
            " Epoch 020\n",
            "\t[E: 19] train loss = 0.0150\n",
            "  [E: 19] valid loss = 3.9143\n",
            "... Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0662009265204139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKNqc6ewcmo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score as sk_precision\n",
        "def compute_precision(model, l_dataset, l_label_vocab):\n",
        "    all_predictions = list()\n",
        "    all_labels = list()\n",
        "    for indexed_elem in l_dataset:\n",
        "        indexed_in = indexed_elem[\"inputs\"]\n",
        "        indexed_labels = indexed_elem[\"outputs\"]\n",
        "        predictions = model(indexed_in)\n",
        "        predictions = torch.argmax(predictions, -1).view(-1)\n",
        "        labels = indexed_labels.view(-1)\n",
        "        valid_indices = labels != 0\n",
        "        \n",
        "        valid_predictions = predictions[valid_indices]\n",
        "        valid_labels = labels[valid_indices]\n",
        "        \n",
        "        all_predictions.extend(valid_predictions.tolist())\n",
        "        all_labels.extend(valid_labels.tolist())\n",
        "    # global precision. Does take class imbalance into account.\n",
        "    micro_precision = sk_precision(all_labels, all_predictions, average=\"micro\", zero_division=0)\n",
        "    # precision per class and arithmetic average of them. Does not take into account class imbalance.\n",
        "    macro_precision = sk_precision(all_labels, all_predictions, average=\"macro\", zero_division=0)\n",
        "    per_class_precision = sk_precision(all_labels, all_predictions, labels = list(range(len(l_label_vocab))), average=None, zero_division=0)\n",
        "    \n",
        "    return {\"micro_precision\":micro_precision,\n",
        "            \"macro_precision\":macro_precision, \n",
        "            \"per_class_precision\":per_class_precision}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6LCnN-9jFCj",
        "colab_type": "code",
        "outputId": "e3134f20-80e3-4459-a114-e3af9b34bf1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "precisions = compute_precision(postagger, test_dataset, label_vocabulary)\n",
        "per_class_precision = precisions[\"per_class_precision\"]\n",
        "print(\"Micro Precision: {}\\nMacro Precision: {}\".format(precisions[\"micro_precision\"], precisions[\"macro_precision\"]))\n",
        "print(\"Per class Precision:\")\n",
        "for idx_class, precision in sorted(enumerate(per_class_precision), key=lambda elem: -elem[1]):\n",
        "    label = label_vocabulary.itos[idx_class]\n",
        "    print(label, precision)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro Precision: 0.8323316472211231\n",
            "Macro Precision: 0.28714675767783454\n",
            "Per class Precision:\n",
            "O 0.9092737950909858\n",
            "PER 0.12721224218230207\n",
            "LOC 0.0699726462543016\n",
            "ORG 0.042128347183748846\n",
            "<pad> 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRqyl7jzjPP3",
        "colab_type": "code",
        "outputId": "3a50317a-d838-4f90-d73e-223a310b29e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_set_loss = bilstm_trainer.evaluate(test_dataset)\n",
        "print(\"test set loss: {}\".format(test_set_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set loss: 3.94698133547444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boYF1WLEjSbv",
        "colab_type": "code",
        "outputId": "209146b1-e0f7-463a-c1dd-0bb0442cc9ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def print_outputs(l_trainer, l_testset, num_outputs, l_vocabulary, l_label_vocabulary):\n",
        "    \n",
        "    for i in range(num_outputs):\n",
        "        print(\"sentence {}\".format(i))\n",
        "        print()\n",
        "        test_elem = l_testset[i]\n",
        "\n",
        "        test_x, test_y = test_elem[\"inputs\"], test_elem[\"outputs\"]\n",
        "        \n",
        "        logits, predictions = l_trainer.predict(test_x.unsqueeze(0))\n",
        "        \n",
        "        decoded_labels = POSTaggingDataset.decode_output(logits, l_label_vocabulary)[0]\n",
        "        test_y = test_y.tolist()\n",
        "        print(\"token\\t\\tinput\\t\\tgold\\t\\tprediction\")\n",
        "        print(\"-\"*100)\n",
        "        for raw_elem, idx, label, predicted_label in zip(l_testset.get_raw_element(i), test_x.tolist(), test_y, decoded_labels):\n",
        "            if idx == 0:\n",
        "                break\n",
        "            print(\"{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(raw_elem[\"form\"], l_vocabulary.itos[idx], l_label_vocabulary.itos[label], predicted_label))\n",
        "        print(\"=\"*30)\n",
        "\n",
        "print_outputs(bilstm_trainer, testset, 3, vocabulary, label_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence 0\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "however\t\thowever\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "on\t\ton\t\tO\t\tO\n",
            "may\t\tmay\t\tO\t\tO\n",
            "8th\t\t8th\t\tO\t\tPER\n",
            ",\t\t,\t\tO\t\tO\n",
            "2010\t\t2010\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "sighting\t\tsighting\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "gray\t\tgray\t\tO\t\tO\n",
            "whale\t\twhale\t\tO\t\tO\n",
            "was\t\twas\t\tO\t\tO\n",
            "confirmed\t\tconfirmed\t\tO\t\tO\n",
            "off\t\toff\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "coast\t\tcoast\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "israel\t\tisrael\t\tLOC\t\tO\n",
            "in\t\tin\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "mediterranean\t\tmediterranean\t\tLOC\t\tO\n",
            "sea\t\tsea\t\tLOC\t\tO\n",
            ".\t\t.\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "leading\t\tleading\t\tO\t\tO\n",
            "some\t\tsome\t\tO\t\tO\n",
            "scientists\t\tscientists\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "think\t\tthink\t\tO\t\tLOC\n",
            "they\t\tthey\t\tO\t\tO\n",
            "might\t\tmight\t\tO\t\tO\n",
            "be\t\tbe\t\tO\t\tO\n",
            "repopulating\t\trepopulating\t\tO\t\tO\n",
            "old\t\told\t\tO\t\tO\n",
            "breeding\t\tbreeding\t\tO\t\tPER\n",
            "grounds\t\tgrounds\t\tO\t\tLOC\n",
            "that\t\tthat\t\tO\t\tO\n",
            "have\t\thave\t\tO\t\tO\n",
            "not\t\tnot\t\tO\t\tO\n",
            "been\t\tbeen\t\tO\t\tO\n",
            "used\t\tused\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "centuries\t\tcenturies\t\tO\t\tPER\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n",
            "sentence 1\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "the\t\tthe\t\tO\t\tO\n",
            "plot\t\tplot\t\tO\t\tO\n",
            "focuses\t\tfocuses\t\tO\t\tPER\n",
            "on\t\ton\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "brutal\t\tbrutal\t\tO\t\tO\n",
            "and\t\tand\t\tO\t\tO\n",
            "cunning\t\tcunning\t\tO\t\tO\n",
            "group\t\tgroup\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "recently\t\trecently\t\tO\t\tO\n",
            "escaped\t\tescaped\t\tO\t\tO\n",
            "replicants\t\treplicants\t\tO\t\tLOC\n",
            "hiding\t\thiding\t\tO\t\tO\n",
            "in\t\tin\t\tO\t\tO\n",
            "l.a.\t\tl.a.\t\tLOC\t\tO\n",
            "and\t\tand\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "semi-retired\t\tsemi-retired\t\tO\t\tO\n",
            "blade\t\tblade\t\tO\t\tPER\n",
            "runner\t\trunner\t\tO\t\tPER\n",
            ",\t\t,\t\tO\t\tO\n",
            "rick\t\trick\t\tO\t\tO\n",
            "deckard\t\tdeckard\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "who\t\twho\t\tO\t\tO\n",
            "reluctantly\t\treluctantly\t\tO\t\tO\n",
            "agrees\t\tagrees\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "take\t\ttake\t\tO\t\tO\n",
            "on\t\ton\t\tO\t\tO\n",
            "one\t\tone\t\tO\t\tO\n",
            "more\t\tmore\t\tO\t\tO\n",
            "assignment\t\tassignment\t\tO\t\tO\n",
            "to\t\tto\t\tO\t\tO\n",
            "hunt\t\thunt\t\tO\t\tO\n",
            "them\t\tthem\t\tO\t\tO\n",
            "all\t\tall\t\tO\t\tO\n",
            "down\t\tdown\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "while\t\twhile\t\tO\t\tO\n",
            "searching\t\tsearching\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "own\t\town\t\tO\t\tO\n",
            "identity\t\tidentity\t\tO\t\tORG\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n",
            "sentence 2\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "during\t\tduring\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "old\t\told\t\tO\t\tO\n",
            "age\t\tage\t\tO\t\tO\n",
            ",\t\t,\t\tO\t\tO\n",
            "david\t\tdavid\t\tPER\t\tO\n",
            "spends\t\tspends\t\tO\t\tO\n",
            "his\t\this\t\tO\t\tO\n",
            "nights\t\tnights\t\tO\t\tO\n",
            "with\t\twith\t\tO\t\tO\n",
            "abishag\t\tabishag\t\tPER\t\tPER\n",
            ",\t\t,\t\tO\t\tO\n",
            "a\t\ta\t\tO\t\tO\n",
            "woman\t\twoman\t\tO\t\tO\n",
            "appointed\t\tappointed\t\tO\t\tO\n",
            "for\t\tfor\t\tO\t\tO\n",
            "the\t\tthe\t\tO\t\tO\n",
            "purpose\t\tpurpose\t\tO\t\tO\n",
            "of\t\tof\t\tO\t\tO\n",
            "keeping\t\tkeeping\t\tO\t\tLOC\n",
            "him\t\thim\t\tO\t\tO\n",
            "warm\t\twarm\t\tO\t\tPER\n",
            ".\t\t.\t\tO\t\tO\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q24eXJLnnBco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}