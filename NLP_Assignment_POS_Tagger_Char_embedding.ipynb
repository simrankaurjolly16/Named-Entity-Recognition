{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_POS_Tagger",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwUwhn5KidE",
        "colab_type": "code",
        "outputId": "5ca657b0-a9ad-4fd1-fbd9-94adbf9c35e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md36KiBfKpz5",
        "colab_type": "code",
        "outputId": "0059071d-e352-4535-ac7b-8d9ff3682cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 13:35:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-06 13:35:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-06 13:35:37--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.13MB/s    in 6m 29s  \n",
            "\n",
            "2020-05-06 13:42:07 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd9dhHkNK30A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip glove.6B.zip\n",
        "!pip install conllu\n",
        "!pip install spacy\n",
        "!pip install spacy-conll\n",
        "!git clone https://github.com/pasinit/nlp2020_POStagging_data.git\n",
        "!unzip nlp2020_POStagging_data/r2.2.zip  > /dev/null\n",
        "!rm -rf nlp2020_POStagging_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqKinmmyLHJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "# from torchtext import data\n",
        "# from torchtext.vocab import Vectors\n",
        "from collections import defaultdict\n",
        "from conllu import parse as conllu_parse\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "# from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy_conll import ConllFormatter\n",
        "from spacy_conll import Spacy2ConllParser\n",
        "SEED = 123456\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# from torchtext.vocab import Vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzjvdE7zLPk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_file = \"drive/My Drive/NLP_Assignment/train.tsv\"\n",
        "dev_file = \"drive/My Drive/NLP_Assignment/dev.tsv\"\n",
        "test_file = \"drive/My Drive/NLP_Assignment/test.tsv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtuiOuKX-WbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file2str(file):\n",
        "    input_str = list()\n",
        "    with open(test_file) as tsvfile:\n",
        "        tsvreader = csv.reader(tsvfile, delimiter=\"#\")\n",
        "        for line in tsvreader:\n",
        "            if(len(line[1:]) != 0):\n",
        "                input_str.append(line[1:][0])\n",
        "    return ''.join(input_str)\n",
        "\n",
        "train_str = file2str(training_file)\n",
        "dev_str = file2str(dev_file)\n",
        "test_str = file2str(test_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xE1d9wQ_j9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacyconll = Spacy2ConllParser()\n",
        "spacyconll.parseprint(input_str=train_str, output_file=\"train.conllu\")\n",
        "spacyconll.parseprint(input_str=dev_str, output_file=\"dev.conllu\")\n",
        "spacyconll.parseprint(input_str=test_str, output_file=\"test.conllu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4RamS6NLSka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class man_made_Vocab(object):\n",
        "    \n",
        "    UNK = \"<unk>\"\n",
        "    \n",
        "    def __init__(self, counter, min_freq=1, specials=['<unk>', '<pad>']):\n",
        "        \n",
        "        counter = counter.copy()\n",
        "        self.itos = list()\n",
        "        self.unk_index = None\n",
        "        self.itos = list(specials)\n",
        "        \n",
        "        for tok in specials:\n",
        "            del counter[tok]\n",
        "            \n",
        "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        \n",
        "        for word, freq in words_and_frequencies:\n",
        "            self.itos.append(word)\n",
        "        \n",
        "        if man_made_Vocab.UNK in specials:\n",
        "            unk_index = specials.index(man_made_Vocab.UNK)\n",
        "            self.unk_index = unk_index\n",
        "            self.stoi = defaultdict(self._default_unk_index)\n",
        "        else:\n",
        "            self.stoi = defaultdict()\n",
        "        \n",
        "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
        "        \n",
        "    def _default_unk_index(self):\n",
        "        return self.unk_index\n",
        "\n",
        "    def __getstate__(self):\n",
        "        # avoid picking defaultdict\n",
        "        attrs = dict(self.__dict__)\n",
        "        # cast to regular dict\n",
        "        attrs['stoi'] = dict(self.stoi)\n",
        "        return attrs\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        if state.get(\"unk_index\", None) is None:\n",
        "            stoi = defaultdict()\n",
        "        else:\n",
        "            stoi = defaultdict(self._default_unk_index)\n",
        "        stoi.update(state['stoi'])\n",
        "        state['stoi'] = stoi\n",
        "        self.__dict__.update(state)\n",
        "    \n",
        "    def __getitem__(self, token):\n",
        "        return self.stoi.get(token, self.stoi.get(man_made_Vocab.UNK))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1_L_1AvLZAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTaggingDataset(Dataset):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_file:str, \n",
        "                 window_size:int, \n",
        "                 window_shift:int=-1,\n",
        "                 lowercase=True, \n",
        "                 device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        We assume that the dataset pointed by input_file is already tokenized \n",
        "        and can fit in memory.\n",
        "        Args:\n",
        "            input_file (string): The path to the dataset to be loaded.\n",
        "            window_size (integer): The maximum length of a sentence in terms of \n",
        "            number of tokens.\n",
        "            window_shift (integer): The number of tokens we shift the window \n",
        "            over the sentence. Default value is -1 meaning that the window will\n",
        "            be shifted by window_size.\n",
        "            lowercase (boolean): whether the text has to be lowercased or not.\n",
        "            device (string): device where to put tensors (cpu or cuda).\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_file = input_file\n",
        "        self.window_size = window_size\n",
        "        self.window_shift = window_shift if window_shift > 0 else window_size\n",
        "        self.lowercase = lowercase\n",
        "        with open(input_file) as reader:\n",
        "            # read the entire file with reader.read() e parse it\n",
        "            sentences = conllu_parse(reader.read())\n",
        "        self.device = device\n",
        "        self.data = self.create_windows(sentences)\n",
        "        self.encoded_data = None\n",
        "    \n",
        "    def index_dataset(self, l_vocabulary, l_label_vocabulary):\n",
        "        self.encoded_data = list()\n",
        "        for i in range(len(self.data)):\n",
        "            # for each window\n",
        "            elem = self.data[i]\n",
        "            encoded_elem = torch.LongTensor(self.encode_text(elem, l_vocabulary)).to(self.device)\n",
        "            # for each element d in the elem window (d is a dictionary with the various fields from the CoNLL line) \n",
        "            encoded_labels = torch.LongTensor([l_label_vocabulary.stoi[d[\"upostag\"]] if d is not None \n",
        "                              else l_label_vocabulary.stoi[\"<pad>\"] for d in elem]).to(self.device)\n",
        "            self.encoded_data.append({\"inputs\":encoded_elem, \"outputs\":encoded_labels})\n",
        "\n",
        "    def create_windows(self, sentences):\n",
        "        \"\"\" \n",
        "        Args:\n",
        "            sentences (list of lists of dictionaries, \n",
        "                          where each dictionary represents a word occurrence parsed from a CoNLL line)\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        for sentence in sentences:\n",
        "            if self.lowercase:\n",
        "                for d in sentence:\n",
        "                    # lowers the inflected form\n",
        "                    d[\"form\"] = d[\"form\"].lower()\n",
        "            for i in range(0, len(sentence), self.window_shift):\n",
        "                window = sentence[i:i+self.window_size]\n",
        "                if len(window) < self.window_size:\n",
        "                    window = window + [None]*(self.window_size - len(window))\n",
        "                assert len(window) == self.window_size\n",
        "                data.append(window)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.encoded_data is None:\n",
        "            raise RuntimeError(\"\"\"Trying to retrieve elements but index_dataset\n",
        "            has not been invoked yet! Be sure to invoce index_dataset on this object\n",
        "            before trying to retrieve elements. In case you want to retrieve raw\n",
        "            elements, use the method get_raw_element(idx)\"\"\")\n",
        "        return self.encoded_data[idx]\n",
        "    \n",
        "    def get_raw_element(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_text(sentence:list, l_vocabulary):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sentences (list): list of OrderedDict, each carrying the information about\n",
        "            one token.\n",
        "            l_vocabulary (Vocab): vocabulary with mappings from words to indices and viceversa.\n",
        "        Return:\n",
        "            The method returns a list of indices corresponding to the input tokens.\n",
        "        \"\"\"\n",
        "        indices = list()\n",
        "        for w in sentence:\n",
        "            if w is None:\n",
        "                indices.append(l_vocabulary.stoi[\"<pad>\"])\n",
        "            elif w[\"form\"] in l_vocabulary.stoi: # vocabulary string to integer\n",
        "                indices.append(l_vocabulary.stoi[w[\"form\"]])\n",
        "            else:\n",
        "                indices.append(l_vocabulary.stoi[\"<unk>\"])\n",
        "        return indices\n",
        "    \n",
        "    @staticmethod\n",
        "    def decode_output(outputs:torch.Tensor,\n",
        "                    l_label_vocabulary):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            outputs (Tensor): a Tensor with shape (batch_size, max_len, label_vocab_size)\n",
        "                containing the logits outputed by the neural network.\n",
        "            l_label_vocabulary (Vocab): is the vocabulary containing the mapping from\n",
        "            a string label to its corresponding index and vice versa\n",
        "        Output:\n",
        "            The method returns a list of batch_size length where each element is a list\n",
        "            of labels, one for each input token.\n",
        "        \"\"\"\n",
        "        max_indices = torch.argmax(outputs, -1).tolist() # shape = (batch_size, max_len)\n",
        "        predictions = list()\n",
        "        for indices in max_indices:\n",
        "            # vocabulary integer to string is used to obtain the corresponding word from the max index\n",
        "            predictions.append([l_label_vocabulary.itos[i] for i in indices])\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJXup0GGLcAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(dataset, min_freq=1):\n",
        "    counter = Counter()\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        # for each token in the sentence viewed as a dictionary of items from the CoNLL line\n",
        "        for token in dataset.get_raw_element(i):\n",
        "            if token is not None:\n",
        "                counter[token[\"form\"]]+=1\n",
        "    # we add special tokens for handling padding and unknown words at testing time.\n",
        "    return man_made_Vocab(counter, min_freq=min_freq, specials=['<pad>', '<unk>'])\n",
        "\n",
        "def build_label_vocab(dataset):\n",
        "    counter = Counter()\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        for token in dataset.get_raw_element(i):\n",
        "            if token is not None:\n",
        "                counter[token[\"upostag\"]]+=1\n",
        "    # No <unk> token for labels.\n",
        "    return man_made_Vocab(counter, specials=['<pad>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpfhwrCU6ybB",
        "colab_type": "code",
        "outputId": "8cb9d859-a1d0-4bbe-d13c-361f8860eecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "window_size, window_shift = 100, 100\n",
        "dataset = POSTaggingDataset(\"train.conllu\", window_size, window_shift)\n",
        "vocabulary = build_vocab(dataset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(dataset)\n",
        "dataset.index_dataset(vocabulary, label_vocabulary)\n",
        "print(len(vocabulary))\n",
        "print(len(label_vocabulary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16016/16016 [00:00<00:00, 46893.31it/s]\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 68484.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34799\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJsU6aRBLeD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import six\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "def reporthook(t):\n",
        "    \"\"\"https://github.com/tqdm/tqdm\"\"\"\n",
        "    last_b = [0]\n",
        "\n",
        "    def inner(b=1, bsize=1, tsize=None):\n",
        "        \"\"\"\n",
        "        b: int, optional\n",
        "        Number of blocks just transferred [default: 1].\n",
        "        bsize: int, optional\n",
        "        Size of each block (in tqdm units) [default: 1].\n",
        "        tsize: int, optional\n",
        "        Total size (in tqdm units). If [default: None] remains unchanged.\n",
        "        \"\"\"\n",
        "        if tsize is not None:\n",
        "            t.total = tsize\n",
        "        t.update((b - last_b[0]) * bsize)\n",
        "        last_b[0] = b\n",
        "    return inner\n",
        "  \n",
        "def _infer_shape(f):\n",
        "    num_lines, vector_dim = 0, None\n",
        "    for line in f:\n",
        "        if vector_dim is None:\n",
        "            row = line.rstrip().split(b\" \")\n",
        "            vector = row[1:]\n",
        "            # Assuming word, [vector] format\n",
        "            if len(vector) > 2:\n",
        "                # The header present in some (w2v) formats contains two elements.\n",
        "                vector_dim = len(vector)\n",
        "                num_lines += 1  # First element read\n",
        "        else:\n",
        "            num_lines += 1\n",
        "    f.seek(0)\n",
        "    return num_lines, vector_dim\n",
        "\n",
        "class Vectorization(object):\n",
        "\n",
        "    def __init__(self, name, cache=None,\n",
        "                 url=None, unk_init=None, max_vectors=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "           name: name of the file that contains the vectors\n",
        "           cache: directory for cached vectors\n",
        "           url: url for download if vectors not found in cache\n",
        "           unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
        "               to zero vectors; can be any function that takes in a Tensor and\n",
        "               returns a Tensor of the same size\n",
        "           max_vectors (int): this can be used to limit the number of\n",
        "               pre-trained vectors loaded.\n",
        "               Most pre-trained vector sets are sorted\n",
        "               in the descending order of word frequency.\n",
        "               Thus, in situations where the entire set doesn't fit in memory,\n",
        "               or is not needed for another reason, passing `max_vectors`\n",
        "               can limit the size of the loaded set.\n",
        "        \"\"\"\n",
        "        cache = '.vector_cache' if cache is None else cache\n",
        "        self.itos = None\n",
        "        self.stoi = None\n",
        "        self.vectors = None\n",
        "        self.dim = None\n",
        "        self.unk_init = torch.Tensor.zero_ if unk_init is None else unk_init\n",
        "        self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        if token in self.stoi:\n",
        "            return self.vectors[self.stoi[token]]\n",
        "        else:\n",
        "            return self.unk_init(torch.Tensor(self.dim))\n",
        "\n",
        "    def cache(self, name, cache, url=None, max_vectors=None):\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        if os.path.isfile(name):\n",
        "            path = name\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = os.path.join(cache, os.path.basename(name)) + file_suffix\n",
        "        else:\n",
        "            path = os.path.join(cache, name)\n",
        "            if max_vectors:\n",
        "                file_suffix = '_{}.pt'.format(max_vectors)\n",
        "            else:\n",
        "                file_suffix = '.pt'\n",
        "            path_pt = path + file_suffix\n",
        "\n",
        "        if not os.path.isfile(path_pt):\n",
        "            if not os.path.isfile(path) and url:\n",
        "                # logger.info('Downloading vectors from {}'.format(url))\n",
        "                if not os.path.exists(cache):\n",
        "                    os.makedirs(cache)\n",
        "                dest = os.path.join(cache, os.path.basename(url))\n",
        "                if not os.path.isfile(dest):\n",
        "                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n",
        "                        try:\n",
        "                            urlretrieve(url, dest, reporthook=reporthook(t))\n",
        "                        except KeyboardInterrupt as e:  # remove the partial zip file\n",
        "                            os.remove(dest)\n",
        "                            raise e\n",
        "                # logger.info('Extracting vectors into {}'.format(cache))\n",
        "                ext = os.path.splitext(dest)[1][1:]\n",
        "                if ext == 'zip':\n",
        "                    with zipfile.ZipFile(dest, \"r\") as zf:\n",
        "                        zf.extractall(cache)\n",
        "                elif ext == 'gz':\n",
        "                    if dest.endswith('.tar.gz'):\n",
        "                        with tarfile.open(dest, 'r:gz') as tar:\n",
        "                            tar.extractall(path=cache)\n",
        "            if not os.path.isfile(path):\n",
        "                raise RuntimeError('no vectors found at {}'.format(path))\n",
        "\n",
        "            # logger.info(\"Loading vectors from {}\".format(path))\n",
        "            ext = os.path.splitext(path)[1][1:]\n",
        "            if ext == 'gz':\n",
        "                open_file = gzip.open\n",
        "            else:\n",
        "                open_file = open\n",
        "\n",
        "            vectors_loaded = 0\n",
        "            with open_file(path, 'rb') as f:\n",
        "                num_lines, dim = _infer_shape(f)\n",
        "                if not max_vectors or max_vectors > num_lines:\n",
        "                    max_vectors = num_lines\n",
        "\n",
        "                itos, vectors, dim = [], torch.zeros((max_vectors, dim)), None\n",
        "\n",
        "                for line in tqdm(f, total=max_vectors):\n",
        "                    # Explicitly splitting on \" \" is important, so we don't\n",
        "                    # get rid of Unicode non-breaking spaces in the vectors.\n",
        "                    entries = line.rstrip().split(b\" \")\n",
        "\n",
        "                    word, entries = entries[0], entries[1:]\n",
        "                    if dim is None and len(entries) > 1:\n",
        "                        dim = len(entries)\n",
        "                    elif len(entries) == 1:\n",
        "                        # logger.warning(\"Skipping token {} with 1-dimensional \"\n",
        "                                      #  \"vector {}; likely a header\".format(word, entries))\n",
        "                        continue\n",
        "                    elif dim != len(entries):\n",
        "                        raise RuntimeError(\n",
        "                            \"Vector for token {} has {} dimensions, but previously \"\n",
        "                            \"read vectors have {} dimensions. All vectors must have \"\n",
        "                            \"the same number of dimensions.\".format(word, len(entries),\n",
        "                                                                    dim))\n",
        "\n",
        "                    try:\n",
        "                        if isinstance(word, six.binary_type):\n",
        "                            word = word.decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        # logger.info(\"Skipping non-UTF8 token {}\".format(repr(word)))\n",
        "                        continue\n",
        "\n",
        "                    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\n",
        "                    vectors_loaded += 1\n",
        "                    itos.append(word)\n",
        "\n",
        "                    if vectors_loaded == max_vectors:\n",
        "                        break\n",
        "\n",
        "            self.itos = itos\n",
        "            self.stoi = {word: i for i, word in enumerate(itos)}\n",
        "            self.vectors = torch.Tensor(vectors).view(-1, dim)\n",
        "            self.dim = dim\n",
        "            # logger.info('Saving vectors to {}'.format(path_pt))\n",
        "            if not os.path.exists(cache):\n",
        "                os.makedirs(cache)\n",
        "            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n",
        "        else:\n",
        "            # logger.info('Loading vectors from {}'.format(path_pt))\n",
        "            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vectors)\n",
        "\n",
        "    def get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n",
        "        \"\"\"Look up embedding vectors of tokens.\n",
        "\n",
        "        Arguments:\n",
        "            tokens: a token or a list of tokens. if `tokens` is a string,\n",
        "                returns a 1-D tensor of shape `self.dim`; if `tokens` is a\n",
        "                list of strings, returns a 2-D tensor of shape=(len(tokens),\n",
        "                self.dim).\n",
        "            lower_case_backup : Whether to look up the token in the lower case.\n",
        "                If False, each token in the original case will be looked up;\n",
        "                if True, each token in the original case will be looked up first,\n",
        "                if not found in the keys of the property `stoi`, the token in the\n",
        "                lower case will be looked up. Default: False.\n",
        "\n",
        "        Examples:\n",
        "            >>> examples = ['chip', 'baby', 'Beautiful']\n",
        "            >>> vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "            >>> ret = vec.get_vecs_by_tokens(tokens, lower_case_backup=True)\n",
        "        \"\"\"\n",
        "        to_reduce = False\n",
        "\n",
        "        if not isinstance(tokens, list):\n",
        "            tokens = [tokens]\n",
        "            to_reduce = True\n",
        "\n",
        "        if not lower_case_backup:\n",
        "            indices = [self[token] for token in tokens]\n",
        "        else:\n",
        "            indices = [self[token] if token in self.stoi\n",
        "                       else self[token.lower()]\n",
        "                       for token in tokens]\n",
        "\n",
        "        vecs = torch.stack(indices)\n",
        "        return vecs[0] if to_reduce else vecs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jPhVHCMbUMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTaggerModel(nn.Module):\n",
        "    # we provide the hyperparameters as input\n",
        "    def __init__(self, hparams):\n",
        "        super(POSTaggerModel, self).__init__()\n",
        "        # Embedding layer: a mat∂rix vocab_size x embedding_dim where each index \n",
        "        # correspond to a word in the vocabulary and the i-th row corresponds to \n",
        "        # a latent representation of the i-th word in the vocabulary.\n",
        "        pprint(params)\n",
        "        self.word_embedding = nn.Embedding(hparams.vocab_size, hparams.embedding_dim)\n",
        "        if hparams.embeddings is not None:\n",
        "            print(\"initializing embeddings from pretrained\")\n",
        "            self.word_embedding.weight.data.copy_(hparams.embeddings)\n",
        "\n",
        "        # LSTM layer: an LSTM neural network that process the input text\n",
        "        # (encoded with word embeddings) from left to right and outputs \n",
        "        # a new **contextual** representation of each word that depend\n",
        "        # on the preciding words.\n",
        "        self.lstm = nn.LSTM(hparams.embedding_dim, hparams.hidden_dim, \n",
        "                            bidirectional=hparams.bidirectional,\n",
        "                            num_layers=hparams.num_layers, \n",
        "                            dropout = hparams.dropout if hparams.num_layers > 1 else 0)\n",
        "        # Hidden layer: transforms the input value/scalar into\n",
        "        # a hidden vector representation.\n",
        "        lstm_output_dim = hparams.hidden_dim if hparams.bidirectional is False else hparams.hidden_dim * 2\n",
        "\n",
        "        # During training, randomly zeroes some of the elements of the \n",
        "        # input tensor with probability hparams.dropout using samples \n",
        "        # from a Bernoulli distribution. Each channel will be zeroed out \n",
        "        # independently on every forward call.\n",
        "        # This has proven to be an effective technique for regularization and \n",
        "        # preventing the co-adaptation of neurons\n",
        "        self.dropout = nn.Dropout(hparams.dropout)\n",
        "        self.classifier = nn.Linear(lstm_output_dim, hparams.num_classes)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embeddings = self.word_embedding(x)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        o, (h, c) = self.lstm(embeddings)\n",
        "        o = self.dropout(o)\n",
        "        output = self.classifier(o)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNBr9gVScZlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "    \"\"\"Utility class to train and evaluate a model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        label_vocab,\n",
        "        log_steps:int=10_000,\n",
        "        log_level:int=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: the model we want to train.\n",
        "            loss_function: the loss_function to minimize.\n",
        "            optimizer: the optimizer used to minimize the loss_function.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.label_vocab = label_vocab\n",
        "        self.log_steps = log_steps\n",
        "        self.log_level = log_level\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def train(self, train_dataset:Dataset, \n",
        "              valid_dataset:Dataset, \n",
        "              epochs:int=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            train_dataset: a Dataset or DatasetLoader instance containing\n",
        "                the training instances.\n",
        "            valid_dataset: a Dataset or DatasetLoader instance used to evaluate\n",
        "                learning progress.\n",
        "            epochs: the number of times to iterate over train_dataset.\n",
        "\n",
        "        Returns:\n",
        "            avg_train_loss: the average training loss on train_dataset over\n",
        "                epochs.\n",
        "        \"\"\"\n",
        "        assert epochs > 1 and isinstance(epochs, int)\n",
        "        if self.log_level > 0:\n",
        "            print('Training ...')\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            if self.log_level > 0:\n",
        "                print(' Epoch {:03d}'.format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for step, sample in enumerate(train_dataset):\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                sample_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += sample_loss.tolist()\n",
        "\n",
        "                if self.log_level > 1 and step % self.log_steps == self.log_steps - 1:\n",
        "                    print('\\t[E: {:2d} @ step {}] current avg loss = {:0.4f}'.format(epoch, step, epoch_loss / (step + 1)))\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "            train_loss += avg_epoch_loss\n",
        "            if self.log_level > 0:\n",
        "                print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            valid_loss = self.evaluate(valid_dataset)\n",
        "            \n",
        "            if self.log_level > 0:\n",
        "                print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
        "\n",
        "        if self.log_level > 0:\n",
        "            print('... Done!')\n",
        "        \n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        return avg_epoch_loss\n",
        "    \n",
        "\n",
        "    def evaluate(self, valid_dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            valid_dataset: the dataset to use to evaluate the model.\n",
        "\n",
        "        Returns:\n",
        "            avg_valid_loss: the average validation loss over valid_dataset.\n",
        "        \"\"\"\n",
        "        valid_loss = 0.0\n",
        "        # set dropout to 0!! Needed when we are in inference mode.\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sample in valid_dataset:\n",
        "                inputs = sample['inputs']\n",
        "                labels = sample['outputs']\n",
        "\n",
        "                predictions = self.model(inputs)\n",
        "                predictions = predictions.view(-1, predictions.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                valid_loss += sample_loss.tolist()\n",
        "        \n",
        "        return valid_loss / len(valid_dataset)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: a tensor of indices.\n",
        "        Returns: \n",
        "            A list containing the predicted POS tag for each token in the\n",
        "            input sentences.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(x)\n",
        "            predictions = torch.argmax(logits, -1)\n",
        "            return logits, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13N_rpKSbYfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HParams():\n",
        "    vocab_size = len(vocabulary)\n",
        "    hidden_dim = 256\n",
        "    embedding_dim = 100\n",
        "    num_classes = len(label_vocabulary) # number of different universal POS tags\n",
        "    bidirectional = True\n",
        "    num_layers = 2\n",
        "    dropout = 0.0\n",
        "    embeddings = None\n",
        "params = HParams()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7WBkOLOMYC4",
        "colab_type": "code",
        "outputId": "3cf49354-c938-4382-9ec8-9eb9c68da6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "vectors = Vectorization(\"glove.6B.100d.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 397212/400000 [00:14<00:00, 27800.96it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNnxpy7zOY09",
        "colab_type": "code",
        "outputId": "14f9b649-41ca-4268-d04b-f1f598b78306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFMnOV9dba_3",
        "colab_type": "code",
        "outputId": "c710f57e-6f11-4835-a8de-fe0dd821cea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# import os\n",
        "# vectors = Vectorization(\"glove.6B.100d.txt\")\n",
        "pretrained_embeddings = torch.randn(len(vocabulary), vectors.dim)\n",
        "initialised = 0\n",
        "for i, w in enumerate(vocabulary.itos):\n",
        "    if w in vectors.stoi:\n",
        "        initialised += 1\n",
        "        vec = vectors.get_vecs_by_tokens(w)\n",
        "        pretrained_embeddings[i] = vec\n",
        "    \n",
        "pretrained_embeddings[vocabulary[\"<pad>\"]] = torch.zeros(vectors.dim)\n",
        "params.embedding_dim=vectors.dim\n",
        "params.embeddings = pretrained_embeddings\n",
        "params.vocab_size = len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-98d5ff07e3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minitialised\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minitialised\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkJV3skwbeyT",
        "colab_type": "code",
        "outputId": "61e75640-be76-454f-f0aa-de6178ea1039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "window_size, window_shift = 100, 100\n",
        "device = \"cuda\"\n",
        "trainingset = POSTaggingDataset(\"train.conllu\", window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(trainingset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(trainingset)\n",
        "trainingset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "devset = POSTaggingDataset(\"dev.conllu\", window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(devset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(devset)\n",
        "devset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "testset = POSTaggingDataset(\"test.conllu\", window_size, window_shift, device=device)\n",
        "vocabulary = build_vocab(testset, min_freq=2)\n",
        "label_vocabulary = build_label_vocab(testset)\n",
        "testset.index_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "train_dataset = DataLoader(trainingset, batch_size=128)\n",
        "valid_dataset = DataLoader(devset, batch_size=128)\n",
        "test_dataset = DataLoader(testset, batch_size=128)\n",
        "\n",
        "postagger = POSTaggerModel(params).cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 29%|██▊       | 4585/16016 [00:00<00:00, 45845.02it/s]\u001b[A\n",
            " 56%|█████▋    | 9022/16016 [00:00<00:00, 45390.90it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 44823.37it/s]\n",
            "\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 39%|███▊      | 6202/16016 [00:00<00:00, 62014.00it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 61505.25it/s]\n",
            "100%|█████████▉| 398881/400000 [00:40<00:00, 18697.64it/s]\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 30%|███       | 4825/16016 [00:00<00:00, 48239.46it/s]\u001b[A\n",
            " 59%|█████▉    | 9434/16016 [00:00<00:00, 47572.61it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 46361.93it/s]\n",
            "\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 39%|███▉      | 6275/16016 [00:00<00:00, 62746.02it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 63657.01it/s]\n",
            "\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 28%|██▊       | 4410/16016 [00:00<00:00, 44092.16it/s]\u001b[A\n",
            " 55%|█████▍    | 8790/16016 [00:00<00:00, 44002.15it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 43285.72it/s]\n",
            "\n",
            "  0%|          | 0/16016 [00:00<?, ?it/s]\u001b[A\n",
            " 39%|███▉      | 6222/16016 [00:00<00:00, 62217.39it/s]\u001b[A\n",
            "100%|██████████| 16016/16016 [00:00<00:00, 61001.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<__main__.HParams object at 0x7fede04de6d8>\n",
            "initializing embeddings from pretrained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSaSjrK8cJhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bilstm_trainer = Trainer(\n",
        "    model = postagger,\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=label_vocabulary[\"<pad>\"]),\n",
        "    optimizer = optim.Adam(postagger.parameters()),\n",
        "    label_vocab=label_vocabulary\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuP9rYKcMS8",
        "colab_type": "code",
        "outputId": "0c27e53f-ae9a-435b-8e64-05399ecf5514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bilstm_trainer.train(train_dataset, valid_dataset, 20) #EPOCH1:23:31"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            " Epoch 001\n",
            "\t[E:  0] train loss = 0.9957\n",
            "  [E:  0] valid loss = 0.3151\n",
            " Epoch 002\n",
            "\t[E:  1] train loss = 0.2717\n",
            "  [E:  1] valid loss = 0.2318\n",
            " Epoch 003\n",
            "\t[E:  2] train loss = 0.2233\n",
            "  [E:  2] valid loss = 0.1983\n",
            " Epoch 004\n",
            "\t[E:  3] train loss = 0.2006\n",
            "  [E:  3] valid loss = 0.1826\n",
            " Epoch 005\n",
            "\t[E:  4] train loss = 0.1848\n",
            "  [E:  4] valid loss = 0.1696\n",
            " Epoch 006\n",
            "\t[E:  5] train loss = 0.1725\n",
            "  [E:  5] valid loss = 0.1580\n",
            " Epoch 007\n",
            "\t[E:  6] train loss = 0.1610\n",
            "  [E:  6] valid loss = 0.1478\n",
            " Epoch 008\n",
            "\t[E:  7] train loss = 0.1490\n",
            "  [E:  7] valid loss = 0.1362\n",
            " Epoch 009\n",
            "\t[E:  8] train loss = 0.1357\n",
            "  [E:  8] valid loss = 0.1236\n",
            " Epoch 010\n",
            "\t[E:  9] train loss = 0.1206\n",
            "  [E:  9] valid loss = 0.1112\n",
            " Epoch 011\n",
            "\t[E: 10] train loss = 0.1051\n",
            "  [E: 10] valid loss = 0.1002\n",
            " Epoch 012\n",
            "\t[E: 11] train loss = 0.0952\n",
            "  [E: 11] valid loss = 0.0970\n",
            " Epoch 013\n",
            "\t[E: 12] train loss = 0.0828\n",
            "  [E: 12] valid loss = 0.0864\n",
            " Epoch 014\n",
            "\t[E: 13] train loss = 0.0678\n",
            "  [E: 13] valid loss = 0.0670\n",
            " Epoch 015\n",
            "\t[E: 14] train loss = 0.0552\n",
            "  [E: 14] valid loss = 0.0538\n",
            " Epoch 016\n",
            "\t[E: 15] train loss = 0.0432\n",
            "  [E: 15] valid loss = 0.0479\n",
            " Epoch 017\n",
            "\t[E: 16] train loss = 0.0350\n",
            "  [E: 16] valid loss = 0.0437\n",
            " Epoch 018\n",
            "\t[E: 17] train loss = 0.0285\n",
            "  [E: 17] valid loss = 0.0427\n",
            " Epoch 019\n",
            "\t[E: 18] train loss = 0.0227\n",
            "  [E: 18] valid loss = 0.0346\n",
            " Epoch 020\n",
            "\t[E: 19] train loss = 0.0193\n",
            "  [E: 19] valid loss = 0.0332\n",
            "... Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15847748909857023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKNqc6ewcmo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score as sk_precision\n",
        "def compute_precision(model, l_dataset, l_label_vocab):\n",
        "    all_predictions = list()\n",
        "    all_labels = list()\n",
        "    for indexed_elem in l_dataset:\n",
        "        indexed_in = indexed_elem[\"inputs\"]\n",
        "        indexed_labels = indexed_elem[\"outputs\"]\n",
        "        predictions = model(indexed_in)\n",
        "        predictions = torch.argmax(predictions, -1).view(-1)\n",
        "        labels = indexed_labels.view(-1)\n",
        "        valid_indices = labels != 0\n",
        "        \n",
        "        valid_predictions = predictions[valid_indices]\n",
        "        valid_labels = labels[valid_indices]\n",
        "        \n",
        "        all_predictions.extend(valid_predictions.tolist())\n",
        "        all_labels.extend(valid_labels.tolist())\n",
        "    # global precision. Does take class imbalance into account.\n",
        "    micro_precision = sk_precision(all_labels, all_predictions, average=\"micro\", zero_division=0)\n",
        "    # precision per class and arithmetic average of them. Does not take into account class imbalance.\n",
        "    macro_precision = sk_precision(all_labels, all_predictions, average=\"macro\", zero_division=0)\n",
        "    per_class_precision = sk_precision(all_labels, all_predictions, labels = list(range(len(l_label_vocab))), average=None, zero_division=0)\n",
        "    \n",
        "    return {\"micro_precision\":micro_precision,\n",
        "            \"macro_precision\":macro_precision, \n",
        "            \"per_class_precision\":per_class_precision}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6LCnN-9jFCj",
        "colab_type": "code",
        "outputId": "8662637b-2ed0-42ab-fb0d-bdc5380b9eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "precisions = compute_precision(postagger, test_dataset, label_vocabulary)\n",
        "per_class_precision = precisions[\"per_class_precision\"]\n",
        "print(\"Micro Precision: {}\\nMacro Precision: {}\".format(precisions[\"micro_precision\"], precisions[\"macro_precision\"]))\n",
        "print(\"Per class Precision:\")\n",
        "for idx_class, precision in sorted(enumerate(per_class_precision), key=lambda elem: -elem[1]):\n",
        "    label = label_vocabulary.itos[idx_class]\n",
        "    print(label, precision)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro Precision: 0.9880359429682091\n",
            "Macro Precision: 0.9891901755521848\n",
            "Per class Precision:\n",
            "INTJ 1.0\n",
            "SPACE 1.0\n",
            "CCONJ 0.9997322145853789\n",
            "AUX 0.9997230300512394\n",
            "PUNCT 0.9994816784603596\n",
            "NUM 0.9983073161557269\n",
            "ADP 0.9977319068019886\n",
            "PRON 0.9969244963862832\n",
            "SYM 0.9952380952380953\n",
            "SCONJ 0.9944151906813468\n",
            "DET 0.9906389336321404\n",
            "NOUN 0.9875847372565976\n",
            "ADV 0.9857366641932558\n",
            "VERB 0.9793920206372937\n",
            "X 0.9791666666666666\n",
            "PROPN 0.9775847518679374\n",
            "ADJ 0.9728089802419507\n",
            "PART 0.9509564770830659\n",
            "<pad> 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRqyl7jzjPP3",
        "colab_type": "code",
        "outputId": "1c7c97e5-c2fd-454d-cbfc-34297b6a2bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_set_loss = bilstm_trainer.evaluate(test_dataset)\n",
        "print(\"test set loss: {}\".format(test_set_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set loss: 0.03317644395532885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boYF1WLEjSbv",
        "colab_type": "code",
        "outputId": "7a9aa634-80c6-40bc-fe7a-7f65bfc2720c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def print_outputs(l_trainer, l_testset, num_outputs, l_vocabulary, l_label_vocabulary):\n",
        "    \n",
        "    for i in range(num_outputs):\n",
        "        print(\"sentence {}\".format(i))\n",
        "        print()\n",
        "        test_elem = l_testset[i]\n",
        "\n",
        "        test_x, test_y = test_elem[\"inputs\"], test_elem[\"outputs\"]\n",
        "        \n",
        "        logits, predictions = l_trainer.predict(test_x.unsqueeze(0))\n",
        "        \n",
        "        decoded_labels = POSTaggingDataset.decode_output(logits, l_label_vocabulary)[0]\n",
        "        test_y = test_y.tolist()\n",
        "        print(\"token\\t\\tinput\\t\\tgold\\t\\tprediction\")\n",
        "        print(\"-\"*100)\n",
        "        for raw_elem, idx, label, predicted_label in zip(l_testset.get_raw_element(i), test_x.tolist(), test_y, decoded_labels):\n",
        "            if idx == 0:\n",
        "                break\n",
        "            print(\"{}\\t\\t{}\\t\\t{}\\t\\t{}\".format(raw_elem[\"form\"], l_vocabulary.itos[idx], l_label_vocabulary.itos[label], predicted_label))\n",
        "        print(\"=\"*30)\n",
        "\n",
        "print_outputs(bilstm_trainer, testset, 10, vocabulary, label_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence 0\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            " \t\t \t\tSPACE\t\tPUNCT\n",
            "however\t\thowever\t\tADV\t\tADV\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "on\t\ton\t\tADP\t\tADP\n",
            "may\t\tmay\t\tPROPN\t\tVERB\n",
            "8th\t\t8th\t\tNOUN\t\tADJ\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "2010\t\t2010\t\tNUM\t\tNUM\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "sighting\t\tsighting\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "gray\t\tgray\t\tADJ\t\tPROPN\n",
            "whale\t\twhale\t\tNOUN\t\tADJ\n",
            "was\t\twas\t\tAUX\t\tAUX\n",
            "confirmed\t\tconfirmed\t\tVERB\t\tVERB\n",
            "off\t\toff\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "coast\t\tcoast\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "israel\t\tisrael\t\tPROPN\t\tPROPN\n",
            "in\t\tin\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "mediterranean\t\tmediterranean\t\tPROPN\t\tPROPN\n",
            "sea\t\tsea\t\tPROPN\t\tPROPN\n",
            ".\t\t.\t\tPROPN\t\tPUNCT\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "leading\t\tleading\t\tVERB\t\tADJ\n",
            "some\t\tsome\t\tDET\t\tDET\n",
            "scientists\t\tscientists\t\tNOUN\t\tNOUN\n",
            "to\t\tto\t\tPART\t\tPART\n",
            "think\t\tthink\t\tVERB\t\tVERB\n",
            "they\t\tthey\t\tPRON\t\tPRON\n",
            "might\t\tmight\t\tVERB\t\tVERB\n",
            "be\t\tbe\t\tAUX\t\tAUX\n",
            "repopulating\t\trepopulating\t\tVERB\t\tVERB\n",
            "old\t\told\t\tADJ\t\tADJ\n",
            "breeding\t\tbreeding\t\tNOUN\t\tNOUN\n",
            "grounds\t\tgrounds\t\tNOUN\t\tNOUN\n",
            "that\t\tthat\t\tDET\t\tDET\n",
            "have\t\thave\t\tAUX\t\tAUX\n",
            "not\t\tnot\t\tPART\t\tPART\n",
            "been\t\tbeen\t\tAUX\t\tAUX\n",
            "used\t\tused\t\tVERB\t\tVERB\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "centuries\t\tcenturies\t\tNOUN\t\tADV\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 1\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "plot\t\tplot\t\tNOUN\t\tNOUN\n",
            "focuses\t\tfocuses\t\tVERB\t\tVERB\n",
            "on\t\ton\t\tADP\t\tADP\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "brutal\t\tbrutal\t\tADJ\t\tADJ\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "cunning\t\tcunning\t\tADJ\t\tDET\n",
            "group\t\tgroup\t\tNOUN\t\tCCONJ\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "recently\t\trecently\t\tADV\t\tADV\n",
            "escaped\t\tescaped\t\tVERB\t\tPRON\n",
            "replicants\t\treplicants\t\tNOUN\t\tNOUN\n",
            "hiding\t\thiding\t\tVERB\t\tADV\n",
            "in\t\tin\t\tADP\t\tADP\n",
            "l.a.\t\tl.a.\t\tPROPN\t\tPROPN\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "semi-retired\t\tsemi-retired\t\tADJ\t\tADJ\n",
            "blade\t\tblade\t\tNOUN\t\tPUNCT\n",
            "runner\t\trunner\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "rick\t\trick\t\tPROPN\t\tPROPN\n",
            "deckard\t\tdeckard\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "who\t\twho\t\tPRON\t\tPRON\n",
            "reluctantly\t\treluctantly\t\tADV\t\tADV\n",
            "agrees\t\tagrees\t\tVERB\t\tVERB\n",
            "to\t\tto\t\tPART\t\tPART\n",
            "take\t\ttake\t\tVERB\t\tVERB\n",
            "on\t\ton\t\tADP\t\tADP\n",
            "one\t\tone\t\tNUM\t\tNUM\n",
            "more\t\tmore\t\tADJ\t\tADV\n",
            "assignment\t\tassignment\t\tNOUN\t\tADJ\n",
            "to\t\tto\t\tPART\t\tPART\n",
            "hunt\t\thunt\t\tVERB\t\tPROPN\n",
            "them\t\tthem\t\tPRON\t\tPRON\n",
            "all\t\tall\t\tDET\t\tDET\n",
            "down\t\tdown\t\tADP\t\tADP\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "while\t\twhile\t\tSCONJ\t\tSCONJ\n",
            "searching\t\tsearching\t\tVERB\t\tVERB\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "his\t\this\t\tDET\t\tDET\n",
            "own\t\town\t\tADJ\t\tADJ\n",
            "identity\t\tidentity\t\tNOUN\t\tNOUN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 2\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "during\t\tduring\t\tADP\t\tADP\n",
            "his\t\this\t\tDET\t\tDET\n",
            "old\t\told\t\tADJ\t\tADJ\n",
            "age\t\tage\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "david\t\tdavid\t\tPROPN\t\tPROPN\n",
            "spends\t\tspends\t\tVERB\t\tVERB\n",
            "his\t\this\t\tDET\t\tDET\n",
            "nights\t\tnights\t\tNOUN\t\tADV\n",
            "with\t\twith\t\tADP\t\tADP\n",
            "abishag\t\tabishag\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "woman\t\twoman\t\tNOUN\t\tNOUN\n",
            "appointed\t\tappointed\t\tVERB\t\tVERB\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "purpose\t\tpurpose\t\tNOUN\t\tCCONJ\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "keeping\t\tkeeping\t\tVERB\t\tVERB\n",
            "him\t\thim\t\tPRON\t\tPRON\n",
            "warm\t\twarm\t\tADJ\t\tADJ\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 3\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "large\t\tlarge\t\tADJ\t\tADJ\n",
            "national\t\tnational\t\tADJ\t\tADJ\n",
            "department\t\tdepartment\t\tNOUN\t\tPUNCT\n",
            "stores\t\tstores\t\tNOUN\t\tNOUN\n",
            "also\t\talso\t\tADV\t\tADV\n",
            "arrived\t\tarrived\t\tVERB\t\tVERB\n",
            "downtown\t\tdowntown\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "including\t\tincluding\t\tVERB\t\tVERB\n",
            "montgomery\t\tmontgomery\t\tPROPN\t\tPROPN\n",
            "ward\t\tward\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "sears\t\tsears\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "j.c.\t\tj.c.\t\tPROPN\t\tPROPN\n",
            "penney\t\tpenney\t\tPROPN\t\tPROPN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 4\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "he\t\the\t\tPRON\t\tPRON\n",
            "ordered\t\tordered\t\tVERB\t\tVERB\n",
            "all\t\tall\t\tDET\t\tDET\n",
            "bridges\t\tbridges\t\tNOUN\t\tNOUN\n",
            "across\t\tacross\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "tiber\t\ttiber\t\tPROPN\t\tPROPN\n",
            "cut\t\tcut\t\tNOUN\t\tADV\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "reportedly\t\treportedly\t\tADV\t\tADV\n",
            "on\t\ton\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "counsel\t\tcounsel\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "gods\t\tgods\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "left\t\tleft\t\tVERB\t\tDET\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "rest\t\trest\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "central\t\tcentral\t\tADJ\t\tADJ\n",
            "italy\t\titaly\t\tPROPN\t\tPROPN\n",
            "undefended\t\tundefended\t\tVERB\t\tADV\n",
            ";\t\t;\t\tPUNCT\t\tPUNCT\n",
            "constantine\t\tconstantine\t\tNOUN\t\tPROPN\n",
            "secured\t\tsecured\t\tVERB\t\tVERB\n",
            "that\t\tthat\t\tDET\t\tDET\n",
            "region\t\tregion\t\tNOUN\t\tNOUN\n",
            "'s\t\t's\t\tPART\t\tPART\n",
            "support\t\tsupport\t\tNOUN\t\tNOUN\n",
            "without\t\twithout\t\tADP\t\tADP\n",
            "challenge\t\tchallenge\t\tNOUN\t\tNOUN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 5\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "having\t\thaving\t\tVERB\t\tVERB\n",
            "spent\t\tspent\t\tVERB\t\tVERB\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "greater\t\tgreater\t\tADJ\t\tADJ\n",
            "part\t\tpart\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "his\t\this\t\tDET\t\tDET\n",
            "early\t\tearly\t\tADJ\t\tADJ\n",
            "life\t\tlife\t\tNOUN\t\tNOUN\n",
            "in\t\tin\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "twilight\t\ttwilight\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "nero\t\tnero\t\tPROPN\t\tPROPN\n",
            "'s\t\t's\t\tPART\t\tPART\n",
            "reign\t\treign\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "his\t\this\t\tDET\t\tDET\n",
            "formative\t\tformative\t\tADJ\t\tADJ\n",
            "years\t\tyears\t\tNOUN\t\tNOUN\n",
            "would\t\twould\t\tVERB\t\tVERB\n",
            "have\t\thave\t\tAUX\t\tAUX\n",
            "been\t\tbeen\t\tAUX\t\tAUX\n",
            "strongly\t\tstrongly\t\tADV\t\tADV\n",
            "influenced\t\tinfluenced\t\tVERB\t\tADV\n",
            "by\t\tby\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "political\t\tpolitical\t\tADJ\t\tADJ\n",
            "turmoil\t\tturmoil\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "60s\t\t60s\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "culminating\t\tculminating\t\tVERB\t\tADV\n",
            "with\t\twith\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "civil\t\tcivil\t\tADJ\t\tADJ\n",
            "war\t\twar\t\tNOUN\t\tPROPN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "69\t\t69\t\tNUM\t\tNUM\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "which\t\twhich\t\tDET\t\tDET\n",
            "brought\t\tbrought\t\tVERB\t\tVERB\n",
            "his\t\this\t\tDET\t\tDET\n",
            "family\t\tfamily\t\tNOUN\t\tCCONJ\n",
            "to\t\tto\t\tADP\t\tPART\n",
            "power\t\tpower\t\tNOUN\t\tNOUN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 6\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "attraction\t\tattraction\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "hastings\t\thastings\t\tPROPN\t\tPROPN\n",
            "as\t\tas\t\tSCONJ\t\tSCONJ\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "tourist\t\ttourist\t\tNOUN\t\tNOUN\n",
            "destination\t\tdestination\t\tNOUN\t\tNOUN\n",
            "continues\t\tcontinues\t\tVERB\t\tADV\n",
            ";\t\t;\t\tPUNCT\t\tPUNCT\n",
            "although\t\talthough\t\tSCONJ\t\tSCONJ\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "numbers\t\tnumbers\t\tNOUN\t\tNOUN\n",
            "of\t\tof\t\tADP\t\tADP\n",
            "hotels\t\thotels\t\tNOUN\t\tNOUN\n",
            "has\t\thas\t\tAUX\t\tAUX\n",
            "decreased\t\tdecreased\t\tVERB\t\tADV\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "it\t\tit\t\tPRON\t\tPRON\n",
            "caters\t\tcaters\t\tVERB\t\tADV\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "wider\t\twider\t\tADJ\t\tADJ\n",
            "tastes\t\ttastes\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "being\t\tbeing\t\tAUX\t\tAUX\n",
            "home\t\thome\t\tADV\t\tADV\n",
            "to\t\tto\t\tADP\t\tPART\n",
            "internationally-based\t\tinternationally-based\t\tADJ\t\tSYM\n",
            "cultural\t\tcultural\t\tADJ\t\tADJ\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "sporting\t\tsporting\t\tVERB\t\tNOUN\n",
            "events\t\tevents\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "such\t\tsuch\t\tADJ\t\tADJ\n",
            "as\t\tas\t\tSCONJ\t\tSCONJ\n",
            "chess\t\tchess\t\tNOUN\t\tPROPN\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "running\t\trunning\t\tNOUN\t\tADV\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 7\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "match\t\tmatch\t\tNOUN\t\tNOUN\n",
            "had\t\thad\t\tAUX\t\tAUX\n",
            "lasted\t\tlasted\t\tVERB\t\tADV\n",
            "an\t\tan\t\tDET\t\tDET\n",
            "unprecedented\t\tunprecedented\t\tADJ\t\tADJ\n",
            "five\t\tfive\t\tNUM\t\tNUM\n",
            "months\t\tmonths\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "with\t\twith\t\tADP\t\tADP\n",
            "five\t\tfive\t\tNUM\t\tNUM\n",
            "wins\t\twins\t\tNOUN\t\tADV\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "karpov\t\tkarpov\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "three\t\tthree\t\tNUM\t\tNUM\n",
            "for\t\tfor\t\tADP\t\tADP\n",
            "kasparov\t\tkasparov\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "and\t\tand\t\tCCONJ\t\tCCONJ\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "staggering\t\tstaggering\t\tADJ\t\tADJ\n",
            "forty\t\tforty\t\tNUM\t\tNUM\n",
            "draws\t\tdraws\t\tNOUN\t\tADV\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 8\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "they\t\tthey\t\tPRON\t\tPRON\n",
            "were\t\twere\t\tAUX\t\tAUX\n",
            "then\t\tthen\t\tADV\t\tADV\n",
            "dispersed\t\tdispersed\t\tVERB\t\tADV\n",
            "or\t\tor\t\tCCONJ\t\tCCONJ\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "according\t\taccording\t\tVERB\t\tVERB\n",
            "to\t\tto\t\tADP\t\tPART\n",
            "some\t\tsome\t\tDET\t\tDET\n",
            "sources\t\tsources\t\tNOUN\t\tNOUN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "sold\t\tsold\t\tVERB\t\tVERB\n",
            "as\t\tas\t\tSCONJ\t\tSCONJ\n",
            "slaves\t\tslaves\t\tNOUN\t\tADV\n",
            "by\t\tby\t\tADP\t\tADP\n",
            "the\t\tthe\t\tDET\t\tDET\n",
            "franks\t\tfranks\t\tPROPN\t\tPROPN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n",
            "sentence 9\n",
            "\n",
            "token\t\tinput\t\tgold\t\tprediction\n",
            "----------------------------------------------------------------------------------------------------\n",
            "he\t\the\t\tPRON\t\tPRON\n",
            "later\t\tlater\t\tADV\t\tADV\n",
            "returned\t\treturned\t\tVERB\t\tVERB\n",
            "to\t\tto\t\tADP\t\tPART\n",
            "his\t\this\t\tDET\t\tDET\n",
            "native\t\tnative\t\tADJ\t\tADJ\n",
            "saudi\t\tsaudi\t\tPROPN\t\tPROPN\n",
            "arabia\t\tarabia\t\tPROPN\t\tPROPN\n",
            ",\t\t,\t\tPUNCT\t\tPUNCT\n",
            "but\t\tbut\t\tCCONJ\t\tCCONJ\n",
            "was\t\twas\t\tAUX\t\tAUX\n",
            "unable\t\tunable\t\tADJ\t\tADJ\n",
            "to\t\tto\t\tPART\t\tPART\n",
            "find\t\tfind\t\tVERB\t\tVERB\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "job\t\tjob\t\tNOUN\t\tPROPN\n",
            "as\t\tas\t\tSCONJ\t\tSCONJ\n",
            "a\t\ta\t\tDET\t\tDET\n",
            "commercial\t\tcommercial\t\tADJ\t\tADJ\n",
            "pilot\t\tpilot\t\tNOUN\t\tNOUN\n",
            ".\t\t.\t\tPUNCT\t\tPUNCT\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q24eXJLnnBco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}